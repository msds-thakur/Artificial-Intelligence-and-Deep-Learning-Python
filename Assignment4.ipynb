{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prabhat Thakur Date 06/07/2019\n",
    "#2019SP_MSDS_458-DL_SEC56 Week10- Assignment4\n",
    "#Create deep neural networks CNN for Gender classcification.\n",
    "#Data : Downloaded from Google \n",
    "#Code Addopted from F. Challot (2018), Deep Learning with Python (Manning) \n",
    "#              and https://github.com/arunponnusamy/gender-detection-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D,MaxPooling2D\n",
    "from keras.layers.core import Dense,Activation,Flatten,Dropout\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import plot_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "import cvlib as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def welcome ():\n",
    "    print (' ' )\n",
    "    print ('******************************************************************************')\n",
    "    print (' ' )\n",
    "    print ('This program is for Gender classcification using CNN network')\n",
    "    print ('deep learning neural network structures. ')\n",
    "    print ('Dataset is gathered from Google Images .')\n",
    "    print ('Classifying Gender: a binary classification example')\n",
    "    print (' ' )\n",
    "    print ('******************************************************************************')\n",
    "    print (' ' )\n",
    "    return()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "epochs = 50\n",
    "lr= 0.001\n",
    "batch_size = 64\n",
    "img_dims = (96,96,3)\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# load image files from the dataset\n",
    "image_files = [f for f in glob.glob('C:/Users/prabhatkumar/Documents/MSDS458Assignment4/gender_dataset_face' + \"/**/*\", recursive=True) if not os.path.isdir(f)] \n",
    "random.shuffle(image_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create groud-truth label from the image path\n",
    "for img in image_files:\n",
    "\n",
    "    image = cv2.imread(img)\n",
    "    \n",
    "    image = cv2.resize(image, (img_dims[0],img_dims[1]))\n",
    "    image = img_to_array(image)\n",
    "    data.append(image)\n",
    "\n",
    "    label = img.split(os.path.sep)[-2]\n",
    "    if label == \"woman\":\n",
    "        label = 1\n",
    "    else:\n",
    "        label = 0\n",
    "        \n",
    "    labels.append([label])\n",
    "\n",
    "# pre-processing\n",
    "data = np.array(data, dtype=\"float\") / 255.0\n",
    "labels = np.array(labels)\n",
    "\n",
    "# split dataset for training and validation\n",
    "(trainX, testX, trainY, testY) = train_test_split(data, labels, test_size=0.2,\n",
    "                                                  random_state=42)\n",
    "trainY = to_categorical(trainY, num_classes=2)\n",
    "testY = to_categorical(testY, num_classes=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augmenting datset \n",
    "aug = ImageDataGenerator(rotation_range=25, width_shift_range=0.1,\n",
    "                         height_shift_range=0.1, shear_range=0.2, zoom_range=0.2,\n",
    "                         horizontal_flip=True, fill_mode=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss (history):\n",
    "    history_dict = history.history\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    epochs = range(1, len(loss) + 1)\n",
    "\n",
    "    # \"bo\" is for \"blue dot\"\n",
    "    plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "    # b is for \"solid blue line\"\n",
    "    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy (history):\n",
    "    history_dict = history.history\n",
    "    \n",
    "    acc = history_dict['acc']\n",
    "    val_acc = history_dict['val_acc']\n",
    "    \n",
    "    plt.clf()\n",
    "    \n",
    "    epochs = range(1, len(acc) + 1)\n",
    "    \n",
    "    plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "    plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallerVGGNet:\n",
    "    @staticmethod\n",
    "    def build(width, height, depth, classes):\n",
    "        model = Sequential()\n",
    "        inputShape = (height, width, depth)\n",
    "        chanDim = -1\n",
    "\n",
    "        if K.image_data_format() == \"channels_first\":\n",
    "            inputShape = (depth, height, width)\n",
    "            chanDim = 1\n",
    "\n",
    "        model.add(Conv2D(32, (3,3), padding=\"same\", input_shape=inputShape))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=chanDim))\n",
    "        model.add(MaxPooling2D(pool_size=(3,3)))\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "        model.add(Conv2D(64, (3,3), padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=chanDim))\n",
    "        model.add(Conv2D(64, (3,3), padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=chanDim))\n",
    "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "        model.add(Conv2D(128, (3,3), padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=chanDim))\n",
    "        model.add(Conv2D(128, (3,3), padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=chanDim))\n",
    "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1024))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.5))\n",
    "\n",
    "        model.add(Dense(classes))\n",
    "        model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model\n",
    "GenderModel = SmallerVGGNet()\n",
    "model = GenderModel.build(width=img_dims[0], height=img_dims[1], depth=img_dims[2],\n",
    "                          classes=2) \n",
    "    \n",
    "# compile the model\n",
    "opt = Adam(lr=lr, decay=lr/epochs)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "28/28 [==============================] - 251s 9s/step - loss: 0.7901 - acc: 0.7116 - val_loss: 0.6170 - val_acc: 0.7727\n",
      "Epoch 2/50\n",
      "28/28 [==============================] - 282s 10s/step - loss: 0.4767 - acc: 0.8003 - val_loss: 0.5124 - val_acc: 0.8084\n",
      "Epoch 3/50\n",
      "28/28 [==============================] - 295s 11s/step - loss: 0.4171 - acc: 0.8306 - val_loss: 0.4303 - val_acc: 0.8323\n",
      "Epoch 4/50\n",
      "28/28 [==============================] - 291s 10s/step - loss: 0.3687 - acc: 0.8457 - val_loss: 0.3714 - val_acc: 0.8604\n",
      "Epoch 5/50\n",
      "28/28 [==============================] - 292s 10s/step - loss: 0.3056 - acc: 0.8758 - val_loss: 0.3352 - val_acc: 0.8766\n",
      "Epoch 6/50\n",
      "28/28 [==============================] - 290s 10s/step - loss: 0.2823 - acc: 0.8895 - val_loss: 0.7001 - val_acc: 0.7522\n",
      "Epoch 7/50\n",
      "28/28 [==============================] - 291s 10s/step - loss: 0.2949 - acc: 0.8784 - val_loss: 0.3991 - val_acc: 0.8496\n",
      "Epoch 8/50\n",
      "28/28 [==============================] - 291s 10s/step - loss: 0.2703 - acc: 0.8923 - val_loss: 0.7261 - val_acc: 0.7825\n",
      "Epoch 9/50\n",
      "28/28 [==============================] - 293s 10s/step - loss: 0.2468 - acc: 0.9016 - val_loss: 0.2413 - val_acc: 0.9134\n",
      "Epoch 10/50\n",
      "28/28 [==============================] - 293s 10s/step - loss: 0.2084 - acc: 0.9112 - val_loss: 0.2783 - val_acc: 0.9069\n",
      "Epoch 11/50\n",
      "28/28 [==============================] - 295s 11s/step - loss: 0.2077 - acc: 0.9214 - val_loss: 0.2333 - val_acc: 0.9134\n",
      "Epoch 12/50\n",
      "28/28 [==============================] - 291s 10s/step - loss: 0.1951 - acc: 0.9200 - val_loss: 0.1548 - val_acc: 0.9459\n",
      "Epoch 13/50\n",
      "28/28 [==============================] - 387s 14s/step - loss: 0.1392 - acc: 0.9461 - val_loss: 0.1521 - val_acc: 0.9470\n",
      "Epoch 14/50\n",
      "28/28 [==============================] - 338s 12s/step - loss: 0.1559 - acc: 0.9372 - val_loss: 0.2084 - val_acc: 0.9188\n",
      "Epoch 15/50\n",
      "28/28 [==============================] - 326s 12s/step - loss: 0.1419 - acc: 0.9422 - val_loss: 0.2852 - val_acc: 0.9145\n",
      "Epoch 16/50\n",
      "28/28 [==============================] - 297s 11s/step - loss: 0.1515 - acc: 0.9366 - val_loss: 0.1819 - val_acc: 0.9351\n",
      "Epoch 17/50\n",
      "28/28 [==============================] - 297s 11s/step - loss: 0.1272 - acc: 0.9461 - val_loss: 0.1296 - val_acc: 0.9567\n",
      "Epoch 18/50\n",
      "28/28 [==============================] - 299s 11s/step - loss: 0.1299 - acc: 0.9492 - val_loss: 0.1422 - val_acc: 0.9578\n",
      "Epoch 19/50\n",
      "28/28 [==============================] - 294s 11s/step - loss: 0.1341 - acc: 0.9458 - val_loss: 0.3050 - val_acc: 0.9199\n",
      "Epoch 20/50\n",
      "28/28 [==============================] - 297s 11s/step - loss: 0.1401 - acc: 0.9411 - val_loss: 0.3833 - val_acc: 0.8961\n",
      "Epoch 21/50\n",
      "28/28 [==============================] - 299s 11s/step - loss: 0.1314 - acc: 0.9500 - val_loss: 0.1575 - val_acc: 0.9524\n",
      "Epoch 22/50\n",
      "28/28 [==============================] - 307s 11s/step - loss: 0.1409 - acc: 0.9392 - val_loss: 0.6067 - val_acc: 0.8063\n",
      "Epoch 23/50\n",
      "28/28 [==============================] - 351s 13s/step - loss: 0.1369 - acc: 0.9426 - val_loss: 0.1131 - val_acc: 0.9654\n",
      "Epoch 24/50\n",
      "28/28 [==============================] - 329s 12s/step - loss: 0.1116 - acc: 0.9565 - val_loss: 0.1225 - val_acc: 0.9643\n",
      "Epoch 25/50\n",
      "28/28 [==============================] - 351s 13s/step - loss: 0.1042 - acc: 0.9602 - val_loss: 0.3453 - val_acc: 0.9069\n",
      "Epoch 26/50\n",
      "28/28 [==============================] - 302s 11s/step - loss: 0.1197 - acc: 0.9497 - val_loss: 0.2213 - val_acc: 0.9264\n",
      "Epoch 27/50\n",
      "28/28 [==============================] - 301s 11s/step - loss: 0.1219 - acc: 0.9507 - val_loss: 0.1932 - val_acc: 0.9351\n",
      "Epoch 28/50\n",
      "28/28 [==============================] - 299s 11s/step - loss: 0.1031 - acc: 0.9623 - val_loss: 0.1532 - val_acc: 0.9600\n",
      "Epoch 29/50\n",
      "28/28 [==============================] - 300s 11s/step - loss: 0.0767 - acc: 0.9710 - val_loss: 0.8300 - val_acc: 0.7695\n",
      "Epoch 30/50\n",
      "28/28 [==============================] - 301s 11s/step - loss: 0.1146 - acc: 0.9548 - val_loss: 1.5824 - val_acc: 0.6450\n",
      "Epoch 31/50\n",
      "28/28 [==============================] - 304s 11s/step - loss: 0.1009 - acc: 0.9586 - val_loss: 1.3542 - val_acc: 0.6840\n",
      "Epoch 32/50\n",
      "28/28 [==============================] - 303s 11s/step - loss: 0.0896 - acc: 0.9668 - val_loss: 0.6586 - val_acc: 0.8149\n",
      "Epoch 33/50\n",
      "28/28 [==============================] - 301s 11s/step - loss: 0.0944 - acc: 0.9628 - val_loss: 0.1279 - val_acc: 0.9610\n",
      "Epoch 34/50\n",
      "28/28 [==============================] - 301s 11s/step - loss: 0.0802 - acc: 0.9701 - val_loss: 0.4365 - val_acc: 0.8626\n",
      "Epoch 35/50\n",
      "28/28 [==============================] - 301s 11s/step - loss: 0.0824 - acc: 0.9704 - val_loss: 0.3964 - val_acc: 0.8701\n",
      "Epoch 36/50\n",
      "28/28 [==============================] - 302s 11s/step - loss: 0.0853 - acc: 0.9657 - val_loss: 0.2727 - val_acc: 0.9167\n",
      "Epoch 37/50\n",
      "28/28 [==============================] - 298s 11s/step - loss: 0.0627 - acc: 0.9771 - val_loss: 0.2675 - val_acc: 0.9123\n",
      "Epoch 38/50\n",
      "28/28 [==============================] - 299s 11s/step - loss: 0.0815 - acc: 0.9689 - val_loss: 0.1562 - val_acc: 0.9372\n",
      "Epoch 39/50\n",
      "28/28 [==============================] - 321s 11s/step - loss: 0.0974 - acc: 0.9594 - val_loss: 0.6415 - val_acc: 0.8344\n",
      "Epoch 40/50\n",
      "28/28 [==============================] - 317s 11s/step - loss: 0.0865 - acc: 0.9676 - val_loss: 0.1376 - val_acc: 0.9632\n",
      "Epoch 41/50\n",
      "28/28 [==============================] - 301s 11s/step - loss: 0.0806 - acc: 0.9699 - val_loss: 0.2251 - val_acc: 0.9340\n",
      "Epoch 42/50\n",
      "28/28 [==============================] - 301s 11s/step - loss: 0.0712 - acc: 0.9713 - val_loss: 0.1671 - val_acc: 0.9491\n",
      "Epoch 43/50\n",
      "28/28 [==============================] - 303s 11s/step - loss: 0.0747 - acc: 0.9688 - val_loss: 0.4062 - val_acc: 0.8864\n",
      "Epoch 44/50\n",
      "28/28 [==============================] - 310s 11s/step - loss: 0.0658 - acc: 0.9741 - val_loss: 0.1177 - val_acc: 0.9686\n",
      "Epoch 45/50\n",
      "28/28 [==============================] - 309s 11s/step - loss: 0.0770 - acc: 0.9710 - val_loss: 0.0842 - val_acc: 0.9719\n",
      "Epoch 46/50\n",
      "28/28 [==============================] - 322s 11s/step - loss: 0.0779 - acc: 0.9707 - val_loss: 0.9979 - val_acc: 0.7381\n",
      "Epoch 47/50\n",
      "28/28 [==============================] - 319s 11s/step - loss: 0.0614 - acc: 0.9760 - val_loss: 0.1032 - val_acc: 0.9675\n",
      "Epoch 48/50\n",
      "28/28 [==============================] - 318s 11s/step - loss: 0.0859 - acc: 0.9680 - val_loss: 0.2377 - val_acc: 0.9545\n",
      "Epoch 49/50\n",
      "28/28 [==============================] - 325s 12s/step - loss: 0.0864 - acc: 0.9654 - val_loss: 0.2761 - val_acc: 0.9102\n",
      "Epoch 50/50\n",
      "28/28 [==============================] - 332s 12s/step - loss: 0.0801 - acc: 0.9680 - val_loss: 0.1114 - val_acc: 0.9729\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "H = model.fit_generator(aug.flow(trainX, trainY, batch_size=batch_size),\n",
    "                        validation_data=(testX,testY),\n",
    "                        steps_per_epoch=len(trainX) // batch_size,\n",
    "                        epochs=epochs, verbose=1)\n",
    "\n",
    "model.save('gender_prediction1.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "******************************************************************************\n",
      " \n",
      "This program is for Gender classcification using CNN network\n",
      "deep learning neural network structures. \n",
      "Dataset is gathered from Google Images .\n",
      "Classifying Gender: a binary classification example\n",
      " \n",
      "******************************************************************************\n",
      " \n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 96, 96, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 96, 96, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 96, 96, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              8389632   \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 2050      \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 8,675,202\n",
      "Trainable params: 8,672,322\n",
      "Non-trainable params: 2,880\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "welcome ()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training/validation loss/accuracy\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "N = epochs\n",
    "plt.plot(np.arange(0,N), H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0,N), H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.arange(0,N), H.history[\"acc\"], label=\"train_acc\")\n",
    "plt.plot(np.arange(0,N), H.history[\"val_acc\"], label=\"val_acc\")\n",
    "\n",
    "plt.title(\"Training Loss and Accuracy\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "\n",
    "plt.show()\n",
    "# save plot to disk\n",
    "plt.savefig('train_val.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydot\n",
    "\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "keras.utils.vis_utils.pydot = pydot\n",
    "\n",
    "plot_model(model, to_file='gender_prediction1.png', show_shapes=True, show_layer_names=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.999957e-01 4.959139e-06]\n",
      "['man', 'woman']\n"
     ]
    }
   ],
   "source": [
    "# Lets test our model\n",
    "# read input image\n",
    "image = cv2.imread('Prabhat.JPG')\n",
    "\n",
    "if image is None:\n",
    "    print(\"Could not read input image\")\n",
    "    exit()\n",
    "\n",
    "# load pre-trained model\n",
    "saved_model = load_model('gender_prediction.model')\n",
    "\n",
    "# detect faces in the image\n",
    "face, confidence = cv.detect_face(image)\n",
    "\n",
    "classes = ['man','woman']\n",
    "\n",
    "# loop through detected faces\n",
    "for idx, f in enumerate(face):\n",
    "\n",
    "     # get corner points of face rectangle       \n",
    "    (startX, startY) = f[0], f[1]\n",
    "    (endX, endY) = f[2], f[3]\n",
    "\n",
    "    # draw rectangle over face\n",
    "    cv2.rectangle(image, (startX,startY), (endX,endY), (0,255,0), 2)\n",
    "\n",
    "    # crop the detected face region\n",
    "    face_crop = np.copy(image[startY:endY,startX:endX])\n",
    "\n",
    "    # preprocessing for gender detection model\n",
    "    face_crop = cv2.resize(face_crop, (96,96))\n",
    "    face_crop = face_crop.astype(\"float\") / 255.0\n",
    "    face_crop = img_to_array(face_crop)\n",
    "    face_crop = np.expand_dims(face_crop, axis=0)\n",
    "\n",
    "    # apply gender detection on face\n",
    "    conf = saved_model.predict(face_crop)[0]\n",
    "    print(conf)\n",
    "    print(classes)\n",
    "\n",
    "    # get label with max accuracy\n",
    "    idx = np.argmax(conf)\n",
    "    label = classes[idx]\n",
    "\n",
    "    label = \"{}: {:.2f}%\".format(label, conf[idx] * 100)\n",
    "\n",
    "    Y = startY - 10 if startY - 10 > 10 else startY + 10\n",
    "\n",
    "    # write label and confidence above face rectangle\n",
    "    cv2.putText(image, label, (startX, Y),  cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                0.7, (0, 255, 0), 2)\n",
    "\n",
    "# display output\n",
    "cv2.imshow(\"gender detection\", image)\n",
    "cv2.imwrite( \"Prabhat_result.jpg\", image );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
